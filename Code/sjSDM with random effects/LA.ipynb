{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import numpy as np, numpy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class Simulation:\n",
    "    X: np.ndarray\n",
    "    Y: np.ndarray\n",
    "    W: np.ndarray\n",
    "    g: np.ndarray\n",
    "    G: np.ndarray\n",
    "    re: np.ndarray\n",
    "    sd_re: float\n",
    "    \n",
    "def simulate(N=100,E=2,G=10, sd_re=1.0):\n",
    "    X = np.random.uniform(-1, 1, [N, E])\n",
    "    W = np.random.normal(0, 1.0, [E, 1])\n",
    "    Y = np.matmul(X, W) + np.random.normal(0.0, 0.8, [N,1])\n",
    "    re = np.random.normal(0, sd_re, [G,1])\n",
    "    g = np.repeat(np.arange(0,G), np.round(N/G))\n",
    "    Y = Y+re[g,:]\n",
    "    return Simulation(X, Y,W, g, G, re, sd_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ray.remote(num_gpus=1,num_cpus=2)\n",
    "def fit_model(data: Simulation, det=True, CAR=False, device = \"cuda:0\", batch_size = 100) -> list:\n",
    "     N, E = data.X.shape\n",
    "     SP = data.Y.shape[1]\n",
    "     X, Y, G, indices = data.X, data.Y, data.G, data.g\n",
    "     dev = torch.device(device)\n",
    "    \n",
    "     XT = torch.tensor(X, dtype=torch.float32, device=torch.device(\"cpu:0\"))\n",
    "     YT = torch.tensor(Y, dtype=torch.float32, device=torch.device(\"cpu:0\"))\n",
    "     indices_T = torch.tensor(indices, dtype=torch.long, device=torch.device(\"cpu:0\"))\n",
    "\n",
    "     # Variables\n",
    "     W = torch.tensor(np.random.normal(0.0,0.001, [XT.shape[1], YT.shape[1]]), dtype=torch.float32, device=dev, requires_grad=True)\n",
    "     scale_log = torch.tensor(1.0, dtype=torch.float32, requires_grad=True, device=dev)\n",
    "     scale_log_normal = torch.tensor(0.0, dtype=torch.float32, requires_grad=True, device=dev)\n",
    "     res = torch.tensor(np.random.normal(0.0,0.001, [G, 1]), dtype=torch.float32, requires_grad=True, device=dev)\n",
    "     soft = lambda t: torch.nn.functional.softplus(t)+0.0001\n",
    "     zero_intercept = torch.zeros([1], dtype=torch.float32, device=dev)\n",
    "     loss2 = torch.zeros([1], dtype=torch.float32, device=dev)\n",
    "\n",
    "     mse = torch.nn.MSELoss(reduction=\"mean\")\n",
    "     \n",
    "     adapt = torch.tensor(np.rint(XT.shape[0]/batch_size).tolist(), dtype=torch.float32, device=dev)\n",
    "     \n",
    "     def ll(res, W, XT, YT, indices_T, scale_log, scale_log_normal):\n",
    "        pred = XT@W+res[indices_T,:]\n",
    "        loss = -torch.distributions.Normal(pred, torch.exp(scale_log_normal)).log_prob(YT).sum()/XT.shape[0]\n",
    "        #loss = mse(YT, pred).mean()\n",
    "        #sigma_noise = torch.exp(scale_log_normal)\n",
    "        #sigma_res = 1./torch.square(torch.exp(scale_log))\n",
    "        # crit_factor = 1.0 / (2 * sigma_noise.square())\n",
    "        loss += -torch.distributions.Normal(zero_intercept, torch.exp(scale_log)).log_prob(res[indices_T.unique()]).sum()/adapt/XT.shape[0]#/XT.shape[0]\n",
    "        #loss += ((res[indices_T.unique()].pow(2.0))*(sigma_res)*0.5).sum()/res[indices_T.unique()].shape[0]#/crit_factor\n",
    "        return loss\n",
    "\n",
    "\n",
    "     optimizer = torch.optim.Adamax([W, scale_log,scale_log_normal], lr = 0.1)\n",
    "     optimizer_re = torch.optim.Adamax([res], lr = 0.1)\n",
    "\n",
    "     dataset = torch.utils.data.TensorDataset(XT, YT, indices_T)\n",
    "     dataLoader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "     \n",
    "     for _ in range(20):\n",
    "          for x, y, inds in dataLoader:\n",
    "               optimizer_re.zero_grad()\n",
    "               loss = ll(res, W.detach(), x.to(dev), y.to(dev), inds.to(dev),torch.tensor(100.0, dtype=torch.float32), scale_log_normal.detach() )\n",
    "               loss.backward(  )\n",
    "               optimizer_re.step()\n",
    "     optimizer_re.zero_grad()\n",
    "     print(torch.std(res))\n",
    "\n",
    "     for i in range(40):\n",
    "          if i > 1:\n",
    "               #print(torch.std(res))\n",
    "               for x, y, inds in dataLoader:\n",
    "                    optimizer_re.zero_grad()\n",
    "                    loss = ll(res, W, x.to(dev), y.to(dev), inds.to(dev), scale_log.detach(), scale_log_normal.detach())#/x.shape[0]\n",
    "                    loss.backward()\n",
    "                    optimizer_re.step()\n",
    "               optimizer_re.zero_grad()\n",
    "          \n",
    "          if i > 0:\n",
    "               for _ in range(3):\n",
    "                    for x, y, inds in dataLoader:\n",
    "                         optimizer.zero_grad()\n",
    "                         loss = ll(res, W, x.to(dev), y.to(dev), inds.to(dev),scale_log, scale_log_normal)#/x.shape[0]\n",
    "                         if det is True:\n",
    "                              #loss.backward(  retain_graph=True )\n",
    "                              #gg = torch.autograd.grad(loss, res, retain_graph=True, create_graph=True)[0]\n",
    "                              #gg=gg[gg.nonzero(as_tuple=True)].reshape([-1,1])\n",
    "                              #logDA = torch.reciprocal(gg**2).sqrt().reshape([-1]).diag().inverse().logdet()\n",
    "                              #loss2 = (-0.5*logDA)#/x.shape[0]\n",
    "                              hess = torch.autograd.functional.hessian(lambda res: ll(res, W, x.to(dev), y.to(dev), inds.to(dev),scale_log, scale_log_normal), res, create_graph=True).squeeze()\n",
    "                              ind2 = inds.unique()\n",
    "                              D_tmp = hess.index_select(0, ind2).index_select(1, ind2)\n",
    "                              const_val = torch.eye(ind2.shape[0], device=dev, dtype=torch.float32)*0.01\n",
    "                              logDA=(D_tmp+const_val).inverse().logdet()\n",
    "                              loss2 = ((ind2.shape[0]*0.5*torch.log((2*torch.tensor(3.14, dtype=torch.float32))) - 0.5*logDA))/adapt/x.shape[0]\n",
    "                              loss+=loss2\n",
    "                         loss = loss\n",
    "                         loss.backward()\n",
    "                         optimizer.step()\n",
    "          \n",
    "          optimizer.zero_grad()\n",
    "          \n",
    "\n",
    "\n",
    "          if i % 20 == 0:\n",
    "               print([loss.item(), loss2])\n",
    "               \n",
    "     return [(torch.exp(scale_log)).cpu().data.numpy().tolist(), \n",
    "             (torch.exp(scale_log_normal)).cpu().data.numpy().tolist(), \n",
    "             W.cpu().data.numpy()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.289999999999999\n",
      "         Mixed Linear Model Regression Results\n",
      "========================================================\n",
      "Model:            MixedLM Dependent Variable: Y         \n",
      "No. Observations: 1000    Method:             ML        \n",
      "No. Groups:       50      Scale:              0.6364    \n",
      "Min. group size:  20      Log-Likelihood:     -1323.5319\n",
      "Max. group size:  20      Converged:          Yes       \n",
      "Mean group size:  20.0                                  \n",
      "--------------------------------------------------------\n",
      "             Coef.  Std.Err.    z    P>|z| [0.025 0.975]\n",
      "--------------------------------------------------------\n",
      "X1           -1.395    0.045 -30.859 0.000 -1.484 -1.306\n",
      "X2            0.223    0.046   4.891 0.000  0.134  0.313\n",
      "Group Var     5.869    1.517                            \n",
      "========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = simulate(N=1000, G = 50, sd_re=2.3)\n",
    "print(2.3**2)\n",
    "dataset = pd.DataFrame(np.concatenate([data.X, data.Y, np.reshape(data.g, [data.g.shape[0],1])], axis = 1),\n",
    "                       columns = [\"X1\", \"X2\", \"Y\", \"g\"]\n",
    "                      )\n",
    "md = smf.mixedlm(\"Y~0+X1+X2\", dataset, groups=dataset[\"g\"], )\n",
    "mdf = md.fit(reml=False)\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2997, grad_fn=<StdBackward0>)\n",
      "[inf, tensor([0.])]\n",
      "tensor(175.1028, grad_fn=<LogdetBackward0>)\n",
      "tensor(181.4552, grad_fn=<LogdetBackward0>)\n",
      "tensor(186.3749, grad_fn=<LogdetBackward0>)\n",
      "tensor(190.2208, grad_fn=<LogdetBackward0>)\n",
      "tensor(193.2012, grad_fn=<LogdetBackward0>)\n",
      "tensor(195.5276, grad_fn=<LogdetBackward0>)\n",
      "tensor(197.3473, grad_fn=<LogdetBackward0>)\n",
      "tensor(198.7539, grad_fn=<LogdetBackward0>)\n",
      "tensor(199.8382, grad_fn=<LogdetBackward0>)\n",
      "tensor(200.6704, grad_fn=<LogdetBackward0>)\n",
      "tensor(201.2995, grad_fn=<LogdetBackward0>)\n",
      "tensor(201.7719, grad_fn=<LogdetBackward0>)\n",
      "tensor(202.1229, grad_fn=<LogdetBackward0>)\n",
      "tensor(202.3760, grad_fn=<LogdetBackward0>)\n",
      "tensor(202.5502, grad_fn=<LogdetBackward0>)\n",
      "tensor(202.6577, grad_fn=<LogdetBackward0>)\n",
      "tensor(202.7038, grad_fn=<LogdetBackward0>)\n",
      "tensor(202.6926, grad_fn=<LogdetBackward0>)\n",
      "tensor(202.6261, grad_fn=<LogdetBackward0>)\n",
      "tensor(202.5042, grad_fn=<LogdetBackward0>)\n",
      "tensor(202.3284, grad_fn=<LogdetBackward0>)\n",
      "tensor(202.1013, grad_fn=<LogdetBackward0>)\n",
      "tensor(201.8234, grad_fn=<LogdetBackward0>)\n",
      "tensor(201.4984, grad_fn=<LogdetBackward0>)\n",
      "tensor(201.1310, grad_fn=<LogdetBackward0>)\n",
      "tensor(200.7227, grad_fn=<LogdetBackward0>)\n",
      "tensor(200.2790, grad_fn=<LogdetBackward0>)\n",
      "tensor(199.8050, grad_fn=<LogdetBackward0>)\n",
      "tensor(199.3027, grad_fn=<LogdetBackward0>)\n",
      "tensor(198.7776, grad_fn=<LogdetBackward0>)\n",
      "tensor(198.2352, grad_fn=<LogdetBackward0>)\n",
      "tensor(197.6769, grad_fn=<LogdetBackward0>)\n",
      "tensor(197.1081, grad_fn=<LogdetBackward0>)\n",
      "tensor(196.5340, grad_fn=<LogdetBackward0>)\n",
      "tensor(195.9553, grad_fn=<LogdetBackward0>)\n",
      "tensor(195.3772, grad_fn=<LogdetBackward0>)\n",
      "tensor(194.8043, grad_fn=<LogdetBackward0>)\n",
      "tensor(194.2368, grad_fn=<LogdetBackward0>)\n",
      "tensor(193.6790, grad_fn=<LogdetBackward0>)\n",
      "tensor(193.1351, grad_fn=<LogdetBackward0>)\n",
      "tensor(192.6045, grad_fn=<LogdetBackward0>)\n",
      "tensor(192.0908, grad_fn=<LogdetBackward0>)\n",
      "tensor(191.5974, grad_fn=<LogdetBackward0>)\n",
      "tensor(191.1226, grad_fn=<LogdetBackward0>)\n",
      "tensor(190.6696, grad_fn=<LogdetBackward0>)\n",
      "tensor(190.2406, grad_fn=<LogdetBackward0>)\n",
      "tensor(189.8331, grad_fn=<LogdetBackward0>)\n",
      "tensor(189.4492, grad_fn=<LogdetBackward0>)\n",
      "tensor(189.0907, grad_fn=<LogdetBackward0>)\n",
      "tensor(188.7537, grad_fn=<LogdetBackward0>)\n",
      "tensor(188.4399, grad_fn=<LogdetBackward0>)\n",
      "tensor(188.1501, grad_fn=<LogdetBackward0>)\n",
      "tensor(187.8799, grad_fn=<LogdetBackward0>)\n",
      "tensor(187.6303, grad_fn=<LogdetBackward0>)\n",
      "tensor(187.4016, grad_fn=<LogdetBackward0>)\n",
      "tensor(187.1890, grad_fn=<LogdetBackward0>)\n",
      "tensor(186.9930, grad_fn=<LogdetBackward0>)\n",
      "tensor(186.8141, grad_fn=<LogdetBackward0>)\n",
      "tensor(186.6468, grad_fn=<LogdetBackward0>)\n",
      "tensor(186.4917, grad_fn=<LogdetBackward0>)\n",
      "[1.641299843788147, tensor(-0.0473, grad_fn=<DivBackward0>)]\n",
      "tensor(186.3492, grad_fn=<LogdetBackward0>)\n",
      "tensor(186.2140, grad_fn=<LogdetBackward0>)\n",
      "tensor(186.0867, grad_fn=<LogdetBackward0>)\n",
      "tensor(185.9677, grad_fn=<LogdetBackward0>)\n",
      "tensor(185.8521, grad_fn=<LogdetBackward0>)\n",
      "tensor(185.7405, grad_fn=<LogdetBackward0>)\n",
      "tensor(185.6338, grad_fn=<LogdetBackward0>)\n",
      "tensor(185.5270, grad_fn=<LogdetBackward0>)\n",
      "tensor(185.4211, grad_fn=<LogdetBackward0>)\n",
      "tensor(185.3172, grad_fn=<LogdetBackward0>)\n",
      "tensor(185.2108, grad_fn=<LogdetBackward0>)\n",
      "tensor(185.1030, grad_fn=<LogdetBackward0>)\n",
      "tensor(184.9951, grad_fn=<LogdetBackward0>)\n",
      "tensor(184.8828, grad_fn=<LogdetBackward0>)\n",
      "tensor(184.7678, grad_fn=<LogdetBackward0>)\n",
      "tensor(184.6514, grad_fn=<LogdetBackward0>)\n",
      "tensor(184.5295, grad_fn=<LogdetBackward0>)\n",
      "tensor(184.4041, grad_fn=<LogdetBackward0>)\n",
      "tensor(184.2766, grad_fn=<LogdetBackward0>)\n",
      "tensor(184.1432, grad_fn=<LogdetBackward0>)\n",
      "tensor(184.0059, grad_fn=<LogdetBackward0>)\n",
      "tensor(183.8665, grad_fn=<LogdetBackward0>)\n",
      "tensor(183.7212, grad_fn=<LogdetBackward0>)\n",
      "tensor(183.5721, grad_fn=<LogdetBackward0>)\n",
      "tensor(183.4210, grad_fn=<LogdetBackward0>)\n",
      "tensor(183.2644, grad_fn=<LogdetBackward0>)\n",
      "tensor(183.1045, grad_fn=<LogdetBackward0>)\n",
      "tensor(182.9430, grad_fn=<LogdetBackward0>)\n",
      "tensor(182.7766, grad_fn=<LogdetBackward0>)\n",
      "tensor(182.6073, grad_fn=<LogdetBackward0>)\n",
      "tensor(182.4371, grad_fn=<LogdetBackward0>)\n",
      "tensor(182.2626, grad_fn=<LogdetBackward0>)\n",
      "tensor(182.0859, grad_fn=<LogdetBackward0>)\n",
      "tensor(181.9089, grad_fn=<LogdetBackward0>)\n",
      "tensor(181.7283, grad_fn=<LogdetBackward0>)\n",
      "tensor(181.5461, grad_fn=<LogdetBackward0>)\n",
      "tensor(181.3644, grad_fn=<LogdetBackward0>)\n",
      "tensor(181.1796, grad_fn=<LogdetBackward0>)\n",
      "tensor(180.9939, grad_fn=<LogdetBackward0>)\n",
      "tensor(180.8093, grad_fn=<LogdetBackward0>)\n",
      "tensor(180.6221, grad_fn=<LogdetBackward0>)\n",
      "tensor(180.4346, grad_fn=<LogdetBackward0>)\n",
      "tensor(180.2487, grad_fn=<LogdetBackward0>)\n",
      "tensor(180.0608, grad_fn=<LogdetBackward0>)\n",
      "tensor(179.8730, grad_fn=<LogdetBackward0>)\n",
      "tensor(179.6872, grad_fn=<LogdetBackward0>)\n",
      "tensor(179.4998, grad_fn=<LogdetBackward0>)\n",
      "tensor(179.3128, grad_fn=<LogdetBackward0>)\n",
      "tensor(179.1282, grad_fn=<LogdetBackward0>)\n",
      "tensor(178.9422, grad_fn=<LogdetBackward0>)\n",
      "tensor(178.7569, grad_fn=<LogdetBackward0>)\n",
      "tensor(178.5742, grad_fn=<LogdetBackward0>)\n",
      "tensor(178.3903, grad_fn=<LogdetBackward0>)\n",
      "tensor(178.2073, grad_fn=<LogdetBackward0>)\n",
      "tensor(178.0269, grad_fn=<LogdetBackward0>)\n",
      "tensor(177.8454, grad_fn=<LogdetBackward0>)\n",
      "tensor(177.6649, grad_fn=<LogdetBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9286086559295654,\n",
       " 1.0407851934432983,\n",
       " array([[-1.3586684 ],\n",
       "        [ 0.22159573]], dtype=float32)]"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(data, device=\"cpu:0\", det=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17998728],\n",
       "       [-1.49548422]])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.W\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2c8e16c7ccab309b6c7ff727be01f1772ce77360f8d9c2b6c3a69dbab5f4903"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
