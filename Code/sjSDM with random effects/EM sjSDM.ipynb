{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a375515",
   "metadata": {},
   "source": [
    "## Playground for random effect integration in sjSDM\n",
    "Dependencies + sjSDM simulation including:\n",
    "\n",
    "* random intercepts per sites\n",
    "* random intercepts per sites and species\n",
    "* random intercepts following CAR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f34bc58-a745-4b0a-b6ac-ff46115fe65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import numpy as np, numpy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class Simulation:\n",
    "    X: np.ndarray\n",
    "    Y: np.ndarray\n",
    "    Sigma: np.ndarray\n",
    "    W: np.ndarray\n",
    "    scale_re: float\n",
    "    g: np.ndarray\n",
    "    G: float\n",
    "    re: np.ndarray\n",
    "    D: np.ndarray\n",
    "    \n",
    "\n",
    "def correlation_from_covariance(covariance):\n",
    "    v = np.sqrt(np.diag(covariance))\n",
    "    outer_v = np.outer(v, v)\n",
    "    correlation = covariance / outer_v\n",
    "    correlation[covariance == 0] = 0\n",
    "    return correlation\n",
    "\n",
    "def _getAplus(A):\n",
    "    eigval, eigvec = np.linalg.eig(A)\n",
    "    Q = np.matrix(eigvec)\n",
    "    xdiag = np.matrix(np.diag(np.maximum(eigval, 0)))\n",
    "    return Q*xdiag*Q.T\n",
    "\n",
    "def _getPs(A, W=None):\n",
    "    W05 = np.matrix(W**.5)\n",
    "    return  W05.I * _getAplus(W05 * A * W05) * W05.I\n",
    "\n",
    "def _getPu(A, W=None):\n",
    "    Aret = np.array(A.copy())\n",
    "    Aret[W > 0] = np.array(W)[W > 0]\n",
    "    return np.matrix(Aret)\n",
    "\n",
    "def nearPD(A, nit=50):\n",
    "    n = A.shape[0]\n",
    "    W = np.identity(n) \n",
    "# W is the matrix used for the norm (assumed to be Identity matrix here)\n",
    "# the algorithm should work for any diagonal W\n",
    "    deltaS = 0\n",
    "    Yk = A.copy()\n",
    "    for k in range(nit):\n",
    "        Rk = Yk - deltaS\n",
    "        Xk = _getPs(Rk, W=W)\n",
    "        deltaS = Xk - Rk\n",
    "        Yk = _getPu(Xk, W=W)\n",
    "    return Yk\n",
    "\n",
    "def simulate_CAR(sides = 10, l = 0.7):\n",
    "    row_coords = np.tile(np.arange(0, sides), sides).reshape([-1, 1])\n",
    "    col_coords = np.tile(np.arange(0, sides), sides).reshape([-1, 1])\n",
    "    d = np.concatenate([row_coords, col_coords], axis= 1)\n",
    "    D = np.sum((d[:, np.newaxis, :] - d[np.newaxis, :, :]) ** 2, axis = -1)\n",
    "    Sigma = nearPD(np.exp(-l*D))+np.eye(sides**2)*0.01\n",
    "    re= np.random.multivariate_normal(np.zeros([sides**2]), cov=Sigma,size=[1])\n",
    "    return D, Sigma, re\n",
    "\n",
    "def simulate(N=100, E=2, SP=3, G = 20, scale_re = 0.5, CAR=False):\n",
    "    if CAR is not True:\n",
    "        re = np.random.normal(0, scale_re, [G,1])\n",
    "        g = np.repeat(np.arange(0,G), np.round(N/G))\n",
    "        D = np.NaN\n",
    "    else:\n",
    "        D, Sigma, re = simulate_CAR(G, scale_re)\n",
    "        g = np.arange(0, G**2)\n",
    "        re = re.reshape([-1, 1])\n",
    "    \n",
    "    X = np.random.uniform(-1, 1, size=[N,E] )\n",
    "    W = np.random.normal(size=[E,SP])\n",
    "    Y = X@W + np.reshape(re[g,:], newshape=[N,1])\n",
    "    \n",
    "    SS = np.random.uniform(-1,1,size=[SP, SP])\n",
    "    SS = correlation_from_covariance(SS@np.transpose(SS))\n",
    "    YY = np.concatenate([np.random.multivariate_normal(Y[i,:], cov = SS, size=[1,]) for i in range(N)], 0)\n",
    "    YY = YY > 0\n",
    "    YY = YY.astype(np.float64)\n",
    "    return Simulation(X, YY, SS, W, scale_re, g, G, re, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd2544",
   "metadata": {},
   "source": [
    "`fit_model` estimates a MVP jSDM based on the sjSDM approach and returns the standard deviation of the random effects and the covariance accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7bbb930-6320-4ab0-9f33-61f6bec25d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(data, \n",
    "              outer_epochs=12, \n",
    "              inner_epochs=3, \n",
    "              device = \"cpu:0\", \n",
    "              optim = \"torch\", \n",
    "              likelihood_type=\"mvp\", \n",
    "              det=True,\n",
    "              CAR=False\n",
    "             ):\n",
    "    E = data.X.shape[1]\n",
    "    N = data.X.shape[0]\n",
    "    SP = data.Y.shape[1]\n",
    "    X, Y, G, indices, D = data.X, data.Y, data.G, data.g, data.D\n",
    "    if CAR is True:\n",
    "        G = G**2\n",
    "    dev = torch.device(device)\n",
    "    XT = torch.tensor(X, dtype=torch.float32, device=dev)\n",
    "    YT = torch.tensor(Y, dtype=torch.float32, device=dev)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    W = torch.tensor(np.random.normal(0., 0.001, size=(XT.shape[1],Y.shape[1])), dtype=torch.float32, requires_grad=True, device=dev)\n",
    "    r_dim = Y.shape[1]\n",
    "    df = int(np.rint(Y.shape[1]/2))\n",
    "    low = -np.sqrt(6.0/(r_dim+df)) # type: ignore\n",
    "    high = np.sqrt(6.0/(r_dim+df)) # type: ignore      \n",
    "    sigma = torch.tensor(np.random.uniform(low, high, [r_dim, df]), requires_grad = True, dtype=torch.float32, device=dev) # type: ignore\n",
    "\n",
    "    @torch.jit.script\n",
    "    def likelihood(mu: torch.Tensor, Ys: torch.Tensor, sigma: torch.Tensor, batch_size: int, sampling: int, df: int, alpha: float, device: str, dtype: torch.dtype):\n",
    "        noise = torch.randn(size = [sampling, batch_size, df], device=torch.device(device), dtype=dtype)\n",
    "        E = torch.sigmoid(   torch.einsum(\"ijk, lk -> ijl\", [noise, sigma]).add(mu).mul(alpha)   ).mul(0.999999).add(0.0000005)\n",
    "        logprob = E.log().mul(Ys).add((1.0 - E).log().mul(1.0 - Ys)).neg().sum(dim = 2).neg()\n",
    "        maxlogprob = logprob.max(dim = 0).values\n",
    "        Eprob = logprob.sub(maxlogprob).exp().mean(dim = 0)\n",
    "        return Eprob.log().neg().sub(maxlogprob)\n",
    "\n",
    "    scale_log = torch.tensor(np.random.normal(0.0,0.001, [1]), dtype=torch.float32, requires_grad=True, device=dev)\n",
    "    res = torch.tensor(np.random.normal(0.0,0.001, [G, 1]), dtype=torch.float32, requires_grad=True, device=dev)\n",
    "    zero_intercept = torch.zeros([1], dtype=torch.float32, device=dev)\n",
    "    zero_CAR = torch.zeros([G], dtype=torch.float32, device=dev)\n",
    "\n",
    "    opt1 = torch.optim.RMSprop([W, scale_log, sigma],lr=0.01)\n",
    "\n",
    "    if CAR is True:\n",
    "        D = torch.tensor(D, dtype=torch.float32, device=dev)\n",
    "    opt2 = torch.optim.LBFGS([res], lr = 0.1)\n",
    "    \n",
    "    const_val = torch.tensor(0.0, dtype=torch.float32, device=dev)\n",
    "    const_cov = torch.eye(G, dtype=torch.float32, device=dev)*0.01\n",
    "\n",
    "    soft = lambda t: torch.nn.functional.softplus(t)+0.0001\n",
    "\n",
    "    indices_T = torch.tensor(indices, dtype=torch.long)\n",
    "    \n",
    "    def re_loss():\n",
    "        if CAR is not True:\n",
    "            return -torch.distributions.Normal(zero_intercept, soft(scale_log)).log_prob(res).sum()\n",
    "        else:\n",
    "            return -torch.distributions.MultivariateNormal(zero_CAR, (-soft(scale_log)*D).exp()+const_cov).log_prob(res.reshape([1,-1])).sum()\n",
    "\n",
    "    \n",
    "    def ll(res, W,sigma, XT, YT, indices_T):\n",
    "        pred = XT@W+res[indices_T,:]#*scale_log.exp()\n",
    "        #loss = -torch.distributions.Normal(loc=pred, scale=soft(scale_log_2) ).log_prob(YT).sum()\n",
    "        if likelihood_type == \"mvp\":\n",
    "            loss = likelihood(pred, YT, sigma, XT.shape[0], 100, df, 1.7012, device, torch.float32).sum()\n",
    "        else:\n",
    "            loss = -torch.distributions.Binomial(total_count=1, probs= torch.sigmoid(pred*1.7012) ).log_prob(YT).sum()\n",
    "        loss += re_loss()\n",
    "        return loss\n",
    "    \n",
    "    def torch_optim(epoch):\n",
    "        if epoch % inner_epochs == 0:\n",
    "            for _ in range(20):\n",
    "                opt2.zero_grad()\n",
    "                loss = ll(res, W,sigma, XT, YT, indices_T)\n",
    "                loss.backward()\n",
    "                opt2.step(lambda: ll(res, W,sigma, XT, YT, indices_T))\n",
    "            opt2.zero_grad()\n",
    "    \n",
    "    def minimize_func(res2):\n",
    "        res2 = torch.tensor(res2, dtype=torch.float32, device=dev).reshape([G, 1])\n",
    "        loss = ll(res2, W,sigma, XT, YT, indices_T)\n",
    "        return loss.cpu().data.numpy()\n",
    "    \n",
    "    def hessian(res2):\n",
    "        res2 = torch.tensor(res2, dtype=torch.float32, requires_grad=True, device=dev)\n",
    "        loss = ll(res2.reshape([G, 1]), W, sigma, XT, YT, indices_T)\n",
    "        loss.backward()\n",
    "        return res2.grad.cpu().data.numpy()\n",
    "    \n",
    "    def scipy_optim(res, epoch, const_val):\n",
    "        if epoch % inner_epochs == 0:\n",
    "            res_new = minimize(\n",
    "                        minimize_func, \n",
    "                        res.cpu().data.numpy().reshape([-1]), \n",
    "                        jac=hessian, \n",
    "                        method=\"BFGS\"\n",
    "                    )\n",
    "            res = torch.tensor(res_new.x, dtype=torch.float32, requires_grad=True, device=dev).reshape([G, 1])\n",
    "            #const_val = torch.tensor( 0.5*(np.log((2*np.pi)**(res_new.hess_inv.shape[0])) - np.log(np.linalg.det(res_new.hess_inv))), dtype=torch.float32, device=dev)\n",
    "            #print(const_val)\n",
    "        return res, const_val\n",
    "    \n",
    "    if det is not True:\n",
    "        const_val = torch.tensor(0.0, dtype=torch.float32, device=dev)\n",
    "    for epoch in range(outer_epochs):\n",
    "        if optim == \"torch\": \n",
    "            torch_optim(epoch)\n",
    "        else:\n",
    "            res, const_val = scipy_optim(res, epoch, const_val)\n",
    "        \n",
    "        sample_indices = np.random.randint(0, XT.shape[0], size = 20)\n",
    "\n",
    "        opt1.zero_grad()        \n",
    "        pred = XT[sample_indices,:]@W+res[indices,:][sample_indices,:]#*scale_log.exp()\n",
    "        loss = likelihood(pred, YT[sample_indices,:], sigma, pred.shape[0], 100, df, 1.7012, device, torch.float32).mean() + const_val\n",
    "        loss += re_loss()\n",
    "        loss.backward()\n",
    "        opt1.step()\n",
    "        opt1.zero_grad()\n",
    "    avg_scale = soft(scale_log).mean().cpu().data.numpy()\n",
    "    avg_acc = np.mean((np.sign(correlation_from_covariance((sigma @ sigma.t()).data.cpu().numpy())) == np.sign(data.Sigma) )[np.triu_indices(SP)])\n",
    "    del opt1\n",
    "    del W\n",
    "    del XT\n",
    "    del YT\n",
    "    del sigma\n",
    "    return avg_scale.tolist(), avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79420b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40456842],\n",
       "       [-0.47768778],\n",
       "       [-0.57108772],\n",
       "       [-0.21552874],\n",
       "       [ 0.71881768],\n",
       "       [ 0.31674885],\n",
       "       [ 1.42935109],\n",
       "       [-0.69193213],\n",
       "       [ 0.18393983],\n",
       "       [ 0.34073885],\n",
       "       [ 0.20831082],\n",
       "       [ 0.05193391],\n",
       "       [-0.06783768],\n",
       "       [-0.24047788],\n",
       "       [ 0.3399139 ],\n",
       "       [-0.52989648],\n",
       "       [-0.02116909],\n",
       "       [ 0.23199976],\n",
       "       [ 0.02096044],\n",
       "       [ 0.3455103 ],\n",
       "       [ 0.06003734],\n",
       "       [-0.93551233],\n",
       "       [-0.39061279],\n",
       "       [-0.34171041],\n",
       "       [ 0.65986416],\n",
       "       [ 0.7608267 ],\n",
       "       [-0.36777635],\n",
       "       [-1.2574748 ],\n",
       "       [-0.25170756],\n",
       "       [-0.3295159 ],\n",
       "       [-0.31556222],\n",
       "       [-0.77594736],\n",
       "       [-0.14177741],\n",
       "       [-0.52532314],\n",
       "       [-0.09569401],\n",
       "       [ 1.15591509],\n",
       "       [-0.08016586],\n",
       "       [ 0.82441512],\n",
       "       [ 0.17998155],\n",
       "       [ 1.08525601],\n",
       "       [ 0.14132331],\n",
       "       [ 0.65178794],\n",
       "       [-0.41840465],\n",
       "       [-0.45944114],\n",
       "       [ 0.04853268],\n",
       "       [ 0.90387567],\n",
       "       [-0.30684973],\n",
       "       [-0.0371868 ],\n",
       "       [ 0.03796143],\n",
       "       [-0.40112712],\n",
       "       [ 0.47779864],\n",
       "       [ 0.44921315],\n",
       "       [ 0.40080424],\n",
       "       [-0.07006795],\n",
       "       [ 0.5423653 ],\n",
       "       [ 0.36965576],\n",
       "       [-0.14301217],\n",
       "       [-0.22619527],\n",
       "       [ 0.06485423],\n",
       "       [ 0.29085903],\n",
       "       [ 0.28651125],\n",
       "       [ 0.24376217],\n",
       "       [ 0.1374285 ],\n",
       "       [ 0.71173097],\n",
       "       [-0.27467802],\n",
       "       [ 0.35474841],\n",
       "       [ 0.19161101],\n",
       "       [ 1.05510804],\n",
       "       [-0.36710234],\n",
       "       [ 0.32709943],\n",
       "       [ 0.06457097],\n",
       "       [-0.02826677],\n",
       "       [-0.18084159],\n",
       "       [-0.21584778],\n",
       "       [-0.12073306],\n",
       "       [ 0.71320465],\n",
       "       [ 0.32572252],\n",
       "       [ 0.758707  ],\n",
       "       [-0.14699749],\n",
       "       [ 0.54849441],\n",
       "       [ 0.6140983 ],\n",
       "       [ 0.26618183],\n",
       "       [-0.58234565],\n",
       "       [-0.60209572],\n",
       "       [-0.34503113],\n",
       "       [ 0.37050768],\n",
       "       [-0.21642521],\n",
       "       [-0.55042142],\n",
       "       [-0.00415465],\n",
       "       [ 0.61650127],\n",
       "       [ 1.0966027 ],\n",
       "       [-0.20510165],\n",
       "       [ 0.20709255],\n",
       "       [ 0.02846783],\n",
       "       [ 0.10907124],\n",
       "       [ 0.21588155],\n",
       "       [-0.1023169 ],\n",
       "       [ 0.78601463],\n",
       "       [-0.02174354],\n",
       "       [ 0.14260426]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = simulate(SP=2,N=5000,scale_re=0.5,G = 100, CAR=False)\n",
    "#fit_model(data, CAR=True)\n",
    "data.re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088c1413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55.80103302001953, 21.666128158569336]\n",
      "[59.56851577758789, 26.515663146972656]\n",
      "[53.23011016845703, 21.068098068237305]\n",
      "[48.70780944824219, 18.318222045898438]\n",
      "[49.03778076171875, 20.532358169555664]\n",
      "[47.15538787841797, 19.405576705932617]\n",
      "[47.09665298461914, 19.003772735595703]\n",
      "[48.80914306640625, 20.8721923828125]\n",
      "[51.730995178222656, 24.477840423583984]\n",
      "[38.06648635864258, 12.463699340820312]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20226380921304, 0.458190164843836, 0.4199224385975236, 1.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(data, CAR=False, det=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e7f56df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(data: Simulation, det=True, CAR=False, device = \"cpu:0\", batch_size=50, ll=\"MVP\", intercept=\"sites\", df=None) -> list:\n",
    "     N, E = data.X.shape\n",
    "     SP = data.Y.shape[1]\n",
    "     X, Y, G, indices, D = data.X, data.Y, data.G, data.g, data.D\n",
    "     dev = torch.device(device)\n",
    "     r_dim = Y.shape[1]\n",
    "     if df is None:\n",
    "          df = int(np.rint(Y.shape[1]/2))\n",
    "     low = -np.sqrt(6.0/(r_dim+df)) \n",
    "     high = np.sqrt(6.0/(r_dim+df))     \n",
    "     XT = torch.tensor(X, dtype=torch.float32, device=torch.device(\"cpu:0\"))\n",
    "     YT = torch.tensor(Y, dtype=torch.float32, device=torch.device(\"cpu:0\"))\n",
    "     indices_T = torch.tensor(indices, dtype=torch.long, device=torch.device(\"cpu:0\"))\n",
    "     init_scale = 10.0\n",
    "     if CAR is True:\n",
    "          G = G**2\n",
    "          D = torch.tensor(D, dtype=torch.float32, device=dev)\n",
    "          init_scale = 0.0\n",
    "     # Variables\n",
    "     W = torch.tensor(np.random.normal(0.0,0.001, [XT.shape[1], YT.shape[1]]), dtype=torch.float32, device=dev, requires_grad=True)\n",
    "     \n",
    "     if intercept is not \"species\":\n",
    "          scale_log = torch.tensor(1.0, dtype=torch.float32, requires_grad=True, device=dev)\n",
    "          res = torch.tensor(np.random.normal(0.0,0.001, [G, 1]), dtype=torch.float32, requires_grad=True, device=dev)\n",
    "     if intercept is \"species\":\n",
    "          res = torch.tensor(np.random.normal(0.0,0.001, [1, SP]), dtype=torch.float32, requires_grad=True, device=dev)\n",
    "          scale_log = torch.tensor(np.random.normal(0.0,0.001, [SP]), dtype=torch.float32, requires_grad=True, device=dev)\n",
    "          init_scale = torch.ones_like(scale_log)\n",
    "     sigma = torch.tensor(np.random.uniform(low, high, [r_dim, df]), requires_grad = True, dtype=torch.float32, device=dev) # type: ignore\n",
    "\n",
    "     soft = lambda t: torch.nn.functional.softplus(t)+0.0001\n",
    "     zero_intercept = torch.zeros([1], dtype=torch.float32, device=dev)\n",
    "     zero_CAR = torch.zeros([G], dtype=torch.float32, device=dev)\n",
    "     loss2 = torch.zeros([1], dtype=torch.float32, device=dev)\n",
    "     adapt = torch.tensor(np.rint(XT.shape[0]/batch_size).tolist(), dtype=torch.float32, device=dev)\n",
    "\n",
    "\n",
    "     @torch.jit.script\n",
    "     def MVP(mu: torch.Tensor, Ys: torch.Tensor, sigma: torch.Tensor, batch_size: int, sampling: int, df: int, alpha: float, device: str, dtype: torch.dtype):\n",
    "         noise = torch.randn(size = [sampling, batch_size, df], device=torch.device(device), dtype=dtype)\n",
    "         E = torch.sigmoid(   torch.einsum(\"ijk, lk -> ijl\", [noise, sigma]).add(mu).mul(alpha)   ).mul(0.999999).add(0.0000005)\n",
    "         logprob = E.log().mul(Ys).add((1.0 - E).log().mul(1.0 - Ys)).neg().sum(dim = 2).neg()\n",
    "         maxlogprob = logprob.max(dim = 0).values\n",
    "         Eprob = logprob.sub(maxlogprob).exp().mean(dim = 0)\n",
    "         return Eprob.log().neg().sub(maxlogprob)\n",
    "    \n",
    "     def Binomial(mu: torch.Tensor, Ys: torch.Tensor, sigma: torch.Tensor, batch_size: int, sampling: int, df: int, alpha: float, device: str, dtype: torch.dtype):\n",
    "          return - torch.distributions.Binomial(1, mu.sigmoid()).log_prob(Ys)\n",
    "     \n",
    "     if ll is \"MVP\":\n",
    "          likelihood = MVP\n",
    "     else:\n",
    "          likelihood = Binomial\n",
    "\n",
    "     def ll(res, W, sigma, XT, YT, indices_T, scale_log):\n",
    "          if intercept is not \"species\":\n",
    "               pred = XT@W+res[indices_T,:]\n",
    "          else:\n",
    "               pred = XT@W+res\n",
    "          loss = likelihood(pred, YT, sigma, XT.shape[0], 100, df, 1.7012, device, torch.float32).sum()/XT.shape[0]\n",
    "          if intercept is \"sites\":\n",
    "               loss += -torch.distributions.Normal(zero_intercept, (scale_log.exp())).log_prob(res[indices_T.unique()]).sum()/adapt/XT.shape[0]\n",
    "          if intercept is \"species\":\n",
    "               loss += -torch.distributions.LowRankMultivariateNormal(torch.zeros([YT.shape[1]], dtype=torch.float32, device=dev), sigma, scale_log.exp() ).log_prob(res).sum()/adapt/XT.shape[0]\n",
    "          if CAR is True:\n",
    "               ind2 = indices_T.unique()\n",
    "               D_tmp = D.index_select(0, ind2).index_select(1, ind2)\n",
    "               const_val = torch.eye(ind2.shape[0], device=dev, dtype=torch.float32)*0.001\n",
    "               loss += -torch.distributions.MultivariateNormal(zero_CAR[ind2], (-(scale_log.exp())*D_tmp).exp()+const_val).log_prob(res[indices_T.unique()].reshape([1,-1])).sum()/adapt/XT.shape[0]\n",
    "          return loss\n",
    "\n",
    "     \n",
    "     optimizer = torch.optim.Adamax([W, scale_log, sigma], lr = 0.03)\n",
    "     optimizer_re = torch.optim.Adamax([res], lr = 0.03)\n",
    "\n",
    "     dataset = torch.utils.data.TensorDataset(XT, YT, indices_T)\n",
    "     dataLoader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "     for _ in range(50):\n",
    "          for x, y, inds in dataLoader:\n",
    "               optimizer_re.zero_grad()\n",
    "               loss = ll(res, W.detach(),sigma.detach(), x.to(dev), y.to(dev), inds.to(dev),torch.tensor(init_scale, dtype=torch.float32, device=dev) )\n",
    "               loss.backward(  )\n",
    "               optimizer_re.step()\n",
    "     optimizer_re.zero_grad()\n",
    "     print(torch.std(res))\n",
    "\n",
    "     for i in range(100):\n",
    "          if i > 0:\n",
    "               #print(torch.std(res))\n",
    "               for x, y, inds in dataLoader:\n",
    "                    optimizer_re.zero_grad()\n",
    "                    loss = ll(res, W.detach(),sigma, x.to(dev), y.to(dev), inds.to(dev), scale_log.detach())#/x.shape[0]\n",
    "                    loss.backward()\n",
    "                    optimizer_re.step()\n",
    "          optimizer_re.zero_grad()\n",
    "\n",
    "          for x, y, inds in dataLoader:\n",
    "               optimizer.zero_grad()\n",
    "               loss = ll(res, W, sigma, x.to(dev), y.to(dev), inds.to(dev), scale_log)#/x.shape[0]\n",
    "               if det is True:\n",
    "                    loss.backward(  retain_graph=True )\n",
    "                    gg = torch.autograd.grad(loss, res, retain_graph=True, create_graph=True)[0]\n",
    "                    gg=gg[gg.nonzero(as_tuple=True)].reshape([-1,1])\n",
    "                    logDA = torch.reciprocal(gg**2*gg.shape[0]).sqrt().reshape([-1]).diag().inverse().logdet()\n",
    "                    #hess = torch.autograd.functional.hessian(lambda res: ll(res, W, sigma, x.to(dev), y.to(dev), inds.to(dev),scale_log), res, create_graph=True).squeeze()\n",
    "                    #ind2 = inds.to(dev).unique()\n",
    "                    #D_tmp = hess.index_select(0, ind2).index_select(1, ind2)\n",
    "                    #const_val = torch.eye(ind2.shape[0], device=dev, dtype=torch.float32)*0.01\n",
    "                    #logDA=(D_tmp+const_val).inverse().logdet()\n",
    "                    loss2 = ((gg.shape[0]*0.5*torch.log((2*torch.tensor(3.14, dtype=torch.float32, device=dev))) - 0.5*logDA))/adapt/x.shape[0]\n",
    "                    #loss2 = gg.shape[0]*0.5*torch.log((2*torch.tensor(3.14, dtype=torch.float32))) - 0.5*logDA\n",
    "                    loss+=loss2\n",
    "               loss = loss\n",
    "               loss.backward()\n",
    "               optimizer.step()\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          #if i % 10 == 0:\n",
    "          #     print([loss.item(), loss2.item()])\n",
    "               \n",
    "     return [(scale_log.exp().mean()).cpu().data.numpy().tolist(), \n",
    "             np.mean((np.sign(correlation_from_covariance((sigma @ sigma.t()).data.cpu().numpy())) == np.sign(data.Sigma) )[np.triu_indices(SP)]), \n",
    "             correlation_from_covariance((sigma @ sigma.t() + scale_log.exp()).data.cpu().numpy()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "83e59b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = simulate(SP = 100, N = 1000  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14e7144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximilianpichler/Library/r-miniconda/envs/r-sjsdm/lib/python3.7/site-packages/ipykernel_launcher.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0823, grad_fn=<StdBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.509558028890751e-06,\n",
       " 0.5134653465346535,\n",
       " array([[1.        , 0.27305794, 0.3846035 , ..., 0.5822861 , 0.5183367 ,\n",
       "         0.31661004],\n",
       "        [0.2729554 , 1.        , 0.69973063, ..., 0.5865577 , 0.51317865,\n",
       "         0.41813707],\n",
       "        [0.38451353, 0.6997175 , 0.9999999 , ..., 0.7276242 , 0.624743  ,\n",
       "         0.5473529 ],\n",
       "        ...,\n",
       "        [0.58230746, 0.58664626, 0.7276993 , ..., 1.0000001 , 0.58745205,\n",
       "         0.64994854],\n",
       "        [0.5183621 , 0.5132928 , 0.62483996, ..., 0.58745027, 1.        ,\n",
       "         0.6510153 ],\n",
       "        [0.316561  , 0.41817302, 0.5473907 , ..., 0.64989865, 0.65095186,\n",
       "         1.0000001 ]], dtype=float32)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(data, ll=\"binomial\", intercept=\"species\", df = 40, batch_size = 200, det = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee74000a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1462, -0.2108,  1.5039, -0.3509,  0.9086]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.distributions.LowRankMultivariateNormal(torch.zeros(5), torch.ones([5,2]), torch.ones([5])).sample([1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
