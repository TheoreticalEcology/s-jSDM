{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "8f34bc58-a745-4b0a-b6ac-ff46115fe65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "@dataclass\n",
    "class Simulation:\n",
    "    X: np.ndarray\n",
    "    Y: np.ndarray\n",
    "    Sigma: np.ndarray\n",
    "    W: np.ndarray\n",
    "    scale_re: float\n",
    "    g: np.ndarray\n",
    "    G: float\n",
    "    re: np.ndarray\n",
    "    D: np.ndarray\n",
    "    \n",
    "\n",
    "def correlation_from_covariance(covariance):\n",
    "    v = np.sqrt(np.diag(covariance))\n",
    "    outer_v = np.outer(v, v)\n",
    "    correlation = covariance / outer_v\n",
    "    correlation[covariance == 0] = 0\n",
    "    return correlation\n",
    "\n",
    "import numpy as np,numpy.linalg\n",
    "\n",
    "def _getAplus(A):\n",
    "    eigval, eigvec = np.linalg.eig(A)\n",
    "    Q = np.matrix(eigvec)\n",
    "    xdiag = np.matrix(np.diag(np.maximum(eigval, 0)))\n",
    "    return Q*xdiag*Q.T\n",
    "\n",
    "def _getPs(A, W=None):\n",
    "    W05 = np.matrix(W**.5)\n",
    "    return  W05.I * _getAplus(W05 * A * W05) * W05.I\n",
    "\n",
    "def _getPu(A, W=None):\n",
    "    Aret = np.array(A.copy())\n",
    "    Aret[W > 0] = np.array(W)[W > 0]\n",
    "    return np.matrix(Aret)\n",
    "\n",
    "def nearPD(A, nit=50):\n",
    "    n = A.shape[0]\n",
    "    W = np.identity(n) \n",
    "# W is the matrix used for the norm (assumed to be Identity matrix here)\n",
    "# the algorithm should work for any diagonal W\n",
    "    deltaS = 0\n",
    "    Yk = A.copy()\n",
    "    for k in range(nit):\n",
    "        Rk = Yk - deltaS\n",
    "        Xk = _getPs(Rk, W=W)\n",
    "        deltaS = Xk - Rk\n",
    "        Yk = _getPu(Xk, W=W)\n",
    "    return Yk\n",
    "\n",
    "def simulate(N=100, E=2, SP=3, G = 20, scale_re = 0.5, CAR=False):\n",
    "    if CAR is not True:\n",
    "        re = np.random.normal(0, scale_re, [G,1])\n",
    "        g = np.repeat(np.arange(0,G), np.round(N/G))\n",
    "        D = np.NaN\n",
    "    else:\n",
    "        D, Sigma, re = simulate_CAR(G, scale_re)\n",
    "        g = np.arange(0, G**2)\n",
    "        re = re.reshape([-1, 1])\n",
    "    \n",
    "    X = np.random.uniform(-1, 1, size=[N,E] )\n",
    "    W = np.random.normal(size=[E,SP])\n",
    "    Y = X@W + np.reshape(re[g,:], newshape=[N,1])\n",
    "    \n",
    "    SS = np.random.uniform(-1,1,size=[SP, SP])\n",
    "    SS = correlation_from_covariance(SS@np.transpose(SS))\n",
    "    YY = np.concatenate([np.random.multivariate_normal(Y[i,:], cov = SS, size=[1,]) for i in range(N)], 0)\n",
    "    YY = YY > 0\n",
    "    YY = YY.astype(np.float64)\n",
    "    return Simulation(X, YY, SS, W, scale_re, g, G, re, D)\n",
    "\n",
    "def simulate_CAR(sides = 10, l = 0.7):\n",
    "    row_coords = np.tile(np.arange(0, sides), sides).reshape([-1, 1])\n",
    "    col_coords = np.tile(np.arange(0, sides), sides).reshape([-1, 1])\n",
    "    d = np.concatenate([row_coords, col_coords], axis= 1)\n",
    "    D = np.sum((d[:, np.newaxis, :] - d[np.newaxis, :, :]) ** 2, axis = -1)\n",
    "    Sigma = nearPD(np.exp(-l*D))+np.eye(sides**2)*0.01\n",
    "    re= np.random.multivariate_normal(np.zeros([sides**2]), cov=Sigma,size=[1])\n",
    "    return D, Sigma, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "2bb720b4-535b-428a-a406-1079548d61a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = simulate(SP=4,N=1000,scale_re=0.01,G = 10)\n",
    "E = data.X.shape[1]\n",
    "N = data.X.shape[0]\n",
    "SP = data.Y.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "b7bbb930-6320-4ab0-9f33-61f6bec25d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(data, \n",
    "              outer_epochs=12, \n",
    "              inner_epochs=3, \n",
    "              device = \"cpu:0\", \n",
    "              optim = \"torch\", \n",
    "              likelihood_type=\"mvp\", \n",
    "              det=True,\n",
    "              CAR=False\n",
    "             ):\n",
    "    E = data.X.shape[1]\n",
    "    N = data.X.shape[0]\n",
    "    SP = data.Y.shape[1]\n",
    "    X, Y, G, indices, D = data.X, data.Y, data.G, data.g, data.D\n",
    "    if CAR is True:\n",
    "        G = G**2\n",
    "    dev = torch.device(device)\n",
    "    XT = torch.tensor(X, dtype=torch.float32, device=dev)\n",
    "    YT = torch.tensor(Y, dtype=torch.float32, device=dev)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    W = torch.tensor(np.random.normal(0., 0.001, size=(XT.shape[1],Y.shape[1])), dtype=torch.float32, requires_grad=True, device=dev)\n",
    "    r_dim = Y.shape[1]\n",
    "    df = int(np.rint(Y.shape[1]/2))\n",
    "    low = -np.sqrt(6.0/(r_dim+df)) # type: ignore\n",
    "    high = np.sqrt(6.0/(r_dim+df)) # type: ignore      \n",
    "    sigma = torch.tensor(np.random.uniform(low, high, [r_dim, df]), requires_grad = True, dtype=torch.float32, device=dev) # type: ignore\n",
    "\n",
    "    #@torch.jit.script\n",
    "    def likelihood(mu: torch.Tensor, Ys: torch.Tensor, sigma: torch.Tensor, batch_size: int, sampling: int, df: int, alpha: float, device: str, dtype: torch.dtype):\n",
    "        noise = torch.randn(size = [sampling, batch_size, df], device=torch.device(device), dtype=dtype)\n",
    "        E = torch.sigmoid(   torch.einsum(\"ijk, lk -> ijl\", [noise, sigma]).add(mu).mul(alpha)   ).mul(0.999999).add(0.0000005)\n",
    "        logprob = E.log().mul(Ys).add((1.0 - E).log().mul(1.0 - Ys)).neg().sum(dim = 2).neg()\n",
    "        maxlogprob = logprob.max(dim = 0).values\n",
    "        Eprob = logprob.sub(maxlogprob).exp().mean(dim = 0)\n",
    "        loss = Eprob.log().neg().sub(maxlogprob)\n",
    "        return loss\n",
    "\n",
    "    res = torch.tensor(np.random.normal(0, 0.01, size=(G, 1)), dtype=torch.float32, requires_grad=True, device=dev)\n",
    "    scale_log = torch.tensor(np.random.normal(0.0,0.001, [1]), dtype=torch.float32, requires_grad=True, device=dev)\n",
    "    zero_intercept = torch.zeros([1], dtype=torch.float32, device=dev)\n",
    "    zero_CAR = torch.zeros([G], dtype=torch.float32, device=dev)\n",
    "\n",
    "    opt1 = torch.optim.RMSprop([W, scale_log, sigma],lr=0.01)\n",
    "\n",
    "    if CAR is True:\n",
    "        D = torch.tensor(D, dtype=torch.float32, device=dev)\n",
    "    opt2 = torch.optim.LBFGS([res], lr = 0.1)\n",
    "    \n",
    "    const_val = torch.tensor(0.0, dtype=torch.float32, device=dev)\n",
    "    const_cov = torch.eye(G, dtype=torch.float32, device=dev)*0.01\n",
    "\n",
    "    soft = lambda t: torch.nn.functional.softplus(t)+0.0001\n",
    "\n",
    "    indices_T = torch.tensor(indices, dtype=torch.long)\n",
    "    \n",
    "    def re_loss():\n",
    "        if CAR is not True:\n",
    "            loss = -torch.distributions.Normal(zero_intercept, soft(scale_log)).log_prob(res).sum()\n",
    "        else:\n",
    "            loss = -torch.distributions.MultivariateNormal(zero_CAR, (-soft(scale_log)*D).exp()+const_cov).log_prob(res.reshape([1,-1])).sum()\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def ll(res, W,sigma, XT, YT, indices_T):\n",
    "        pred = XT@W+res[indices_T,:]#*scale_log.exp()\n",
    "        #loss = -torch.distributions.Normal(loc=pred, scale=soft(scale_log_2) ).log_prob(YT).sum()\n",
    "        if likelihood_type == \"mvp\":\n",
    "            loss = likelihood(pred, YT, sigma, XT.shape[0], 100, df, 1.7012, device, torch.float32).sum()\n",
    "        else:\n",
    "            loss = -torch.distributions.Binomial(total_count=1, probs= torch.sigmoid(pred*1.7012) ).log_prob(YT).sum()\n",
    "        loss += re_loss()\n",
    "        return loss\n",
    "    \n",
    "    def torch_optim(epoch):\n",
    "        if epoch % inner_epochs == 0:\n",
    "            for i in range(20):\n",
    "                opt2.zero_grad()\n",
    "                loss = ll(res, W,sigma, XT, YT, indices_T)\n",
    "                loss.backward()\n",
    "                opt2.step(lambda: ll(res, W,sigma, XT, YT, indices_T))\n",
    "            opt2.zero_grad()\n",
    "    \n",
    "    def minimize_func(res2):\n",
    "        res2 = torch.tensor(res2, dtype=torch.float32, device=dev).reshape([G, 1])\n",
    "        loss = ll(res2, W,sigma, XT, YT, indices_T)\n",
    "        return loss.cpu().data.numpy()\n",
    "    \n",
    "    def hessian(res2):\n",
    "        res2 = torch.tensor(res2, dtype=torch.float32, requires_grad=True, device=dev)\n",
    "        loss = ll(res2.reshape([G, 1]), W, sigma, XT, YT, indices_T)\n",
    "        loss.backward()\n",
    "        return res2.grad.cpu().data.numpy()\n",
    "    \n",
    "    def scipy_optim(res, epoch, const_val):\n",
    "        if epoch % inner_epochs == 0:\n",
    "            res_new = minimize(\n",
    "                        minimize_func, \n",
    "                        res.cpu().data.numpy().reshape([-1]), \n",
    "                        jac=hessian, \n",
    "                        method=\"BFGS\"\n",
    "                    )\n",
    "            res = torch.tensor(res_new.x, dtype=torch.float32, requires_grad=True, device=dev).reshape([G, 1])\n",
    "            #const_val = torch.tensor( 0.5*(np.log((2*np.pi)**(res_new.hess_inv.shape[0])) - np.log(np.linalg.det(res_new.hess_inv))), dtype=torch.float32, device=dev)\n",
    "            #print(const_val)\n",
    "        return res, const_val\n",
    "    \n",
    "    if det is not True:\n",
    "        const_val = torch.tensor(0.0, dtype=torch.float32, device=dev)\n",
    "\n",
    "    for epoch in range(outer_epochs):\n",
    "        if optim == \"torch\": \n",
    "            torch_optim(epoch)\n",
    "        else:\n",
    "            res, const_val = scipy_optim(res, epoch, const_val)\n",
    "        \n",
    "        sample_indices = np.random.randint(0, XT.shape[0], size = 20)\n",
    "\n",
    "        opt1.zero_grad()        \n",
    "        pred = XT[sample_indices,:]@W+res[indices,:][sample_indices,:]#*scale_log.exp()\n",
    "        loss = likelihood(pred, YT[sample_indices,:], sigma, pred.shape[0], 100, df, 1.7012, device, torch.float32).mean() + const_val\n",
    "        loss += re_loss()\n",
    "        loss.backward()\n",
    "        opt1.step()\n",
    "        opt1.zero_grad()\n",
    "    avg_scale = soft(scale_log).mean().cpu().data.numpy()\n",
    "    avg_acc = np.mean((np.sign(correlation_from_covariance((sigma @ sigma.t()).data.cpu().numpy())) == np.sign(data.Sigma) )[np.triu_indices(SP)])\n",
    "    del opt1\n",
    "    del W\n",
    "    del XT\n",
    "    del YT\n",
    "    del sigma\n",
    "    #torch.cuda.empty_cache()\n",
    "    return avg_scale.tolist(), avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "4ea51833-7de6-46a3-a2b2-caa8b64d247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/9lp9691n1g15x6vj_bm1fjbm0000gn/T/ipykernel_72226/3649358109.py:89: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  re= np.random.multivariate_normal(np.zeros([sides**2]), cov=Sigma,size=[1])\n"
     ]
    }
   ],
   "source": [
    "data = simulate(SP=2,N=400,scale_re=0.3,G = 20, CAR=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "35ec11ba-4e1f-4d31-8910-fbd995505155",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6581300497055054, 0.6666666666666666)"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(data, 550, 20, optim=\"scipy\", likelihood_type=\"mvp\", CAR=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "bd167e91-bea3-4ba9-bf79-8ba3004442e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_y = [fit_model(data, 200, 12, optim=\"scipy\", likelihood_type=\"mvp\", det=True)  for _ in range(15)]\n",
    "#results_n = [fit_model(data, 60, 5, optim=\"scipy\", likelihood_type=\"mvp\", det=False) for _ in range(60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "19adb7ac-a9d5-4f97-8621-e47ee8834fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.13961991 0.51632461]\n",
      "[0.3287191 0.0027272]\n"
     ]
    }
   ],
   "source": [
    "print( np.mean(np.asarray( results_y ), axis = 0) )\n",
    "print( np.std(np.asarray( results_y ), axis = 0) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
