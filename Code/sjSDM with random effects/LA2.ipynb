{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import numpy as np, numpy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class Simulation:\n",
    "    X: np.ndarray\n",
    "    Y: np.ndarray\n",
    "    W: np.ndarray\n",
    "    g: np.ndarray\n",
    "    G: np.ndarray\n",
    "    re: np.ndarray\n",
    "    sd_re: float\n",
    "    \n",
    "def simulate(N=100,E=2,G=10, sd_re=1.0):\n",
    "    X = np.random.uniform(-1, 1, [N, E])\n",
    "    W = np.random.normal(0, 1.0, [E, 1])\n",
    "    Y = np.matmul(X, W) + np.random.normal(0.0, 0.8, [N,1])\n",
    "    re = np.random.normal(0, sd_re, [G,1])\n",
    "    g = np.repeat(np.arange(0,G), np.round(N/G))\n",
    "    Y = Y+re[g,:]\n",
    "    return Simulation(X, Y,W, g, G, re, sd_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ray.remote(num_gpus=1,num_cpus=2)\n",
    "def fit_model(data: Simulation, det=True, CAR=False, device = \"cuda:0\", batch_size = 100, epochs = 40) -> list:\n",
    "     N, E = data.X.shape\n",
    "     SP = data.Y.shape[1]\n",
    "     X, Y, G, indices = data.X, data.Y, data.G, data.g\n",
    "     dev = torch.device(device)\n",
    "    \n",
    "     XT = torch.tensor(X, dtype=torch.float32, device=torch.device(\"cpu:0\"))\n",
    "     YT = torch.tensor(Y, dtype=torch.float32, device=torch.device(\"cpu:0\"))\n",
    "     indices_T = torch.tensor(indices, dtype=torch.long, device=torch.device(\"cpu:0\"))\n",
    "\n",
    "     # Variables\n",
    "     W = torch.tensor(np.random.normal(0.0,0.001, [XT.shape[1], YT.shape[1]]), dtype=torch.float32, device=dev, requires_grad=True)\n",
    "     scale_log = torch.tensor(1.0, dtype=torch.float32, requires_grad=True, device=dev)\n",
    "     scale_log_normal = torch.tensor(1.0, dtype=torch.float32, requires_grad=True, device=dev)\n",
    "     res = torch.tensor(np.random.normal(0.0,0.001, [G, 1]), dtype=torch.float32, requires_grad=True, device=dev)\n",
    "     soft = lambda t: torch.nn.functional.softplus(t)+0.0001\n",
    "     zero_intercept = torch.zeros([1], dtype=torch.float32, device=dev)\n",
    "     loss2 = torch.zeros([1], dtype=torch.float32, device=dev)\n",
    "\n",
    "     \n",
    "     adapt = torch.tensor(np.rint(XT.shape[0]/batch_size).tolist(), dtype=torch.float32, device=dev)\n",
    "     const = torch.tensor(np.log(2*(np.pi)), dtype=torch.float32, device=dev)\n",
    "     \n",
    "     def ll(res, W, XT, YT, indices_T, scale_log, scale_log_normal):\n",
    "        pred = XT@W+res[indices_T,:]\n",
    "        loss = -torch.distributions.Normal(pred, torch.clamp(scale_log_normal, 0.00001, 100.)).log_prob(YT).sum()/XT.shape[0]\n",
    "        loss += -torch.distributions.Normal(zero_intercept, torch.clamp(scale_log,0.0001, 100.)).log_prob(res[indices_T.unique()]).sum()/adapt/XT.shape[0]#/G/indices_T.unique().shape[0] #/XT.shape[0]\n",
    "        #loss += ((res[indices_T.unique()].pow(2.0))*(sigma_res)*0.5).sum()/res[indices_T.unique()].shape[0]#/crit_factor\n",
    "        return loss\n",
    "\n",
    "\n",
    "     optimizer = torch.optim.Adamax([W, scale_log,scale_log_normal, res], lr = 0.1)\n",
    "\n",
    "     dataset = torch.utils.data.TensorDataset(XT, YT, indices_T)\n",
    "     dataLoader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "     \n",
    "\n",
    "     for i in range(epochs):\n",
    "\n",
    "        for x, y, inds in dataLoader:\n",
    "             optimizer.zero_grad()\n",
    "             loss = ll(res, W, x.to(dev), y.to(dev), inds.to(dev),scale_log, scale_log_normal)#/x.shape[0]\n",
    "             if det is True:\n",
    "\n",
    "                  hess = torch.autograd.functional.hessian(lambda res: ll(res, W, x.to(dev), y.to(dev), inds.to(dev),scale_log, scale_log_normal), res, create_graph=True).squeeze()\n",
    "                  ind2 = inds.unique()\n",
    "                  \n",
    "                  D_tmp = hess.index_select(0, ind2).index_select(1, ind2)\n",
    "                  const_val = torch.eye(ind2.shape[0], device=dev, dtype=torch.float32)*0.01\n",
    "                  logDA=(D_tmp+const_val).inverse().logdet()/G/indices_T.unique().shape[0]\n",
    "                  loss2 = -(0.5*logDA)/adapt/x.shape[0]\n",
    "                  loss+=loss2 + const*G/2.\n",
    "             loss = loss\n",
    "             loss.backward()\n",
    "             optimizer.step()          \n",
    "\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            print([loss.item(), loss2, scale_log.item()])\n",
    "               \n",
    "     return [(scale_log).cpu().data.numpy().tolist(), \n",
    "             (scale_log_normal).cpu().data.numpy().tolist(), \n",
    "             W.cpu().data.numpy()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.289999999999999\n",
      "         Mixed Linear Model Regression Results\n",
      "=======================================================\n",
      "Model:            MixedLM Dependent Variable: Y        \n",
      "No. Observations: 100     Method:             ML       \n",
      "No. Groups:       20      Scale:              0.6395   \n",
      "Min. group size:  5       Log-Likelihood:     -119.5411\n",
      "Max. group size:  5       Converged:          Yes      \n",
      "Mean group size:  5.0                                  \n",
      "-------------------------------------------------------\n",
      "             Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "-------------------------------------------------------\n",
      "X1            0.460    0.131  3.520 0.000  0.204  0.716\n",
      "X2           -0.495    0.142 -3.494 0.000 -0.773 -0.217\n",
      "Group Var     0.000    0.068                           \n",
      "=======================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximilianpichler/Library/r-miniconda/envs/r-sjsdm/lib/python3.7/site-packages/statsmodels/regression/mixed_linear_model.py:1634: UserWarning: Random effects covariance is singular\n",
      "  warnings.warn(msg)\n",
      "/Users/maximilianpichler/Library/r-miniconda/envs/r-sjsdm/lib/python3.7/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "data = simulate(N=100, G = 20, sd_re=0.0001)\n",
    "print(2.3**2)\n",
    "dataset = pd.DataFrame(np.concatenate([data.X, data.Y, np.reshape(data.g, [data.g.shape[0],1])], axis = 1),\n",
    "                       columns = [\"X1\", \"X2\", \"Y\", \"g\"]\n",
    "                      )\n",
    "md = smf.mixedlm(\"Y~0+X1+X2\", dataset, groups=dataset[\"g\"], )\n",
    "mdf = md.fit(reml=False)\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19.211347579956055, tensor(-0.0407, grad_fn=<DivBackward0>), 0.16554474830627441]\n",
      "[19.595626831054688, tensor(-0.0392, grad_fn=<DivBackward0>), 0.2396908551454544]\n",
      "[19.7680721282959, tensor(-0.0452, grad_fn=<DivBackward0>), 0.2492096722126007]\n",
      "[19.425113677978516, tensor(-0.0500, grad_fn=<DivBackward0>), 0.264719158411026]\n",
      "[19.407373428344727, tensor(-0.0325, grad_fn=<DivBackward0>), 0.24883286654949188]\n",
      "[19.419963836669922, tensor(-0.0313, grad_fn=<DivBackward0>), 0.2635261118412018]\n",
      "[19.501163482666016, tensor(-0.0428, grad_fn=<DivBackward0>), 0.2796635925769806]\n",
      "[19.622589111328125, tensor(-0.0396, grad_fn=<DivBackward0>), 0.24843156337738037]\n",
      "[19.495563507080078, tensor(-0.0342, grad_fn=<DivBackward0>), 0.2557966113090515]\n",
      "[19.30864715576172, tensor(-0.0261, grad_fn=<DivBackward0>), 0.2783925235271454]\n",
      "[19.863971710205078, tensor(-0.0416, grad_fn=<DivBackward0>), 0.27472516894340515]\n",
      "[19.411075592041016, tensor(-0.0586, grad_fn=<DivBackward0>), 0.2736388146877289]\n",
      "[19.387697219848633, tensor(-0.0419, grad_fn=<DivBackward0>), 0.2585306763648987]\n",
      "[19.497350692749023, tensor(-0.0419, grad_fn=<DivBackward0>), 0.2719053328037262]\n",
      "[19.225719451904297, tensor(-0.0431, grad_fn=<DivBackward0>), 0.27921149134635925]\n",
      "[19.212326049804688, tensor(-0.0362, grad_fn=<DivBackward0>), 0.28359779715538025]\n",
      "[19.514448165893555, tensor(-0.0485, grad_fn=<DivBackward0>), 0.26690834760665894]\n",
      "[19.34161376953125, tensor(-0.0579, grad_fn=<DivBackward0>), 0.26124608516693115]\n",
      "[19.220905303955078, tensor(-0.0413, grad_fn=<DivBackward0>), 0.25924375653266907]\n",
      "[19.10179328918457, tensor(-0.0489, grad_fn=<DivBackward0>), 0.2600054442882538]\n",
      "[19.702978134155273, tensor(-0.0311, grad_fn=<DivBackward0>), 0.24796094000339508]\n",
      "[19.24094581604004, tensor(-0.0522, grad_fn=<DivBackward0>), 0.27341172099113464]\n",
      "[19.53255271911621, tensor(-0.0352, grad_fn=<DivBackward0>), 0.28161147236824036]\n",
      "[19.4122371673584, tensor(-0.0433, grad_fn=<DivBackward0>), 0.27649590373039246]\n",
      "[19.399093627929688, tensor(-0.0562, grad_fn=<DivBackward0>), 0.26196035742759705]\n",
      "[19.56724739074707, tensor(-0.0543, grad_fn=<DivBackward0>), 0.24891622364521027]\n",
      "[19.499576568603516, tensor(-0.0264, grad_fn=<DivBackward0>), 0.2665973901748657]\n",
      "[19.326818466186523, tensor(-0.0497, grad_fn=<DivBackward0>), 0.26888352632522583]\n",
      "[19.61318588256836, tensor(-0.0317, grad_fn=<DivBackward0>), 0.25252851843833923]\n",
      "[19.245737075805664, tensor(-0.0541, grad_fn=<DivBackward0>), 0.28420761227607727]\n",
      "[19.521678924560547, tensor(-0.0418, grad_fn=<DivBackward0>), 0.27496537566185]\n",
      "[19.452829360961914, tensor(-0.0413, grad_fn=<DivBackward0>), 0.2727872133255005]\n",
      "[19.288543701171875, tensor(-0.0536, grad_fn=<DivBackward0>), 0.28270673751831055]\n",
      "[19.723072052001953, tensor(-0.0316, grad_fn=<DivBackward0>), 0.27460959553718567]\n",
      "[19.581422805786133, tensor(-0.0512, grad_fn=<DivBackward0>), 0.2688449025154114]\n",
      "[19.211225509643555, tensor(-0.0547, grad_fn=<DivBackward0>), 0.25916722416877747]\n",
      "[19.397743225097656, tensor(-0.0330, grad_fn=<DivBackward0>), 0.2575894296169281]\n",
      "[19.82330322265625, tensor(-0.0531, grad_fn=<DivBackward0>), 0.2629663348197937]\n",
      "[19.54999542236328, tensor(-0.0494, grad_fn=<DivBackward0>), 0.2572793662548065]\n",
      "[19.333608627319336, tensor(-0.0419, grad_fn=<DivBackward0>), 0.2615079879760742]\n",
      "[19.604427337646484, tensor(-0.0528, grad_fn=<DivBackward0>), 0.24547359347343445]\n",
      "[19.62256622314453, tensor(-0.0420, grad_fn=<DivBackward0>), 0.28026023507118225]\n",
      "[19.33084487915039, tensor(-0.0380, grad_fn=<DivBackward0>), 0.24073061347007751]\n",
      "[19.486452102661133, tensor(-0.0501, grad_fn=<DivBackward0>), 0.262470006942749]\n",
      "[19.528244018554688, tensor(-0.0418, grad_fn=<DivBackward0>), 0.2737734019756317]\n",
      "[19.545743942260742, tensor(-0.0505, grad_fn=<DivBackward0>), 0.27096447348594666]\n",
      "[19.403512954711914, tensor(-0.0410, grad_fn=<DivBackward0>), 0.26264235377311707]\n",
      "[19.284034729003906, tensor(-0.0481, grad_fn=<DivBackward0>), 0.26262345910072327]\n",
      "[19.498926162719727, tensor(-0.0426, grad_fn=<DivBackward0>), 0.26067548990249634]\n",
      "[20.08143424987793, tensor(-0.0445, grad_fn=<DivBackward0>), 0.25672534108161926]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2669656574726105,\n",
       " 0.7639532089233398,\n",
       " array([[ 0.4142138 ],\n",
       "        [-0.46700388]], dtype=float32)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(data, device=\"cpu:0\", det=True, batch_size=10, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2c8e16c7ccab309b6c7ff727be01f1772ce77360f8d9c2b6c3a69dbab5f4903"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
