% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sjSDM_configs.R
\name{madgrad}
\alias{madgrad}
\title{madgrad}
\usage{
madgrad(momentum = 0.9, weight_decay = 0, eps = 1e-06)
}
\arguments{
\item{momentum}{strength of momentum}

\item{weight_decay}{l2 penalty on weights}

\item{eps}{epsilon}
}
\value{
Anonymous function that returns optimizer when called.
}
\description{
stochastic gradient descent optimizer
}
\references{
Defazio, A., & Jelassi, S. (2021). Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization. arXiv preprint arXiv:2101.11075.
}
