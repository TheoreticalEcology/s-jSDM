% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sjSDM_configs.R
\name{AdaBound}
\alias{AdaBound}
\title{AdaBound}
\usage{
AdaBound(
  betas = c(0.9, 0.999),
  final_lr = 0.1,
  gamma = 0.001,
  eps = 1e-08,
  weight_decay = 0,
  amsbound = TRUE
)
}
\arguments{
\item{betas}{betas}

\item{final_lr}{eps}

\item{gamma}{small_const}

\item{eps}{eps}

\item{weight_decay}{weight_decay}

\item{amsbound}{amsbound}
}
\value{
Anonymous function that returns optimizer when called.
}
\description{
adaptive gradient methods with dynamic bound of learning rate, see Luo et al., 2019 for details
}
\references{
Luo, L., Xiong, Y., Liu, Y., & Sun, X. (2019). Adaptive gradient methods with dynamic bound of learning rate. arXiv preprint arXiv:1902.09843.
}
